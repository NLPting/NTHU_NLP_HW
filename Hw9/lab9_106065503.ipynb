{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from akl import akl\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgPreps = 'in_favor_of|_|about|after|against|among|as|at|between|behind|by|for|from|in|into|of|on|upon|over|through|to|toward|towarV in favour of\truled in favour ofV in favour of\truled in favour ofds|with'.split('|')\n",
    "otherPreps ='out|down'.split('|')\n",
    "verbpat = ('V; V n; V ord; V oneself; V adj; V -ing; V to v; V v; V that; V wh; V wh to v; V quote; '+              'V so; V not; V as if; V as though; V someway; V together; V as adj; V as to wh; V by amount; '+              'V amount; V by -ing; V in favour of n; V in favour of ing; V n in favour of n; V n in favour of ing; V n n; V n adj; V n -ing; V n to v; V n v n; V n that; '+              'V n wh; V n wh to v; V n quote; V n v-ed; V n someway; V n with together; '+              'V n as adj; V n into -ing; V adv; V and v').split('; ')\n",
    "verbpat += ['V %s n' % prep for prep in pgPreps]+['V n %s n' % prep for prep in verbpat]\n",
    "verbpat += [pat.replace('V ', 'V-ed ') for pat in verbpat]\n",
    "\n",
    "pgNoun = ('N for n to v; N from n that; N from n to v; N from n for n; N in favor of; N in favour of; '+            'N of amount; N of n as n; N of n to n; N of n with n; N on n for n; N on n to v'+            'N that; N to v; N to n that; N to n to v; N with n for n; N with n that; N with n to v').split('; ')\n",
    "pgNoun += pgNoun + ['N %s -ing' % prep for prep in pgPreps ]\n",
    "pgNoun += pgNoun + ['ADJ %s n' % prep for prep in pgPreps if prep != 'of']+ ['N %s -ing' % prep for prep in pgPreps]\n",
    "pgAdj = ('ADJ adj; ADJ and adj; ADJ as to wh; '+        'ADJ enough; ADJ enough for n; ADJ enough for n to v; ADJ enough n; '+        'ADJ enough n for n; ADJ enough n for n to v; ADJ enough n that; ADJ enough to v; '+        'ADJ for n to v; ADJ from n to n; ADJ in color; ADJ -ing; '+        'ADJ in n as n; ADJ in n from n; ADJ in n to n; ADJ in n with n; ADJ in n as n; ADJ n for n'+        'ADJ n to v; ADJ on n for n; ADJ on n to v; ADJ that; ADJ to v; ADJ to n for n; ADJ n for -ing'+        'ADJ wh; ADJ on n for n; ADJ on n to v; ADJ that; ADJ to v; ADJ to n for n; ADJ n for -ing').split('; ')\n",
    "pgAdj += [ 'ADJ %s n'%prep for prep in pgPreps ]\n",
    "pgPatterns = verbpat + pgAdj + pgNoun\n",
    "\n",
    "reservedWords = 'how wh; who wh; what wh; when wh; someway someway; together together; that that'.split('; ')\n",
    "pronOBJ = ['me', 'us', 'you', 'him', 'them']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapHead = dict( [('H-NP', 'N'), ('H-VP', 'V'), ('H-ADJP', 'ADJ'), ('H-ADVP', 'ADV'), ('H-VB', 'V')] )\n",
    "mapRest = dict( [('VBG', 'ing'), ('VBD', 'v-ed'), ('VBN', 'v-ed'), ('VB', 'v'), ('NN', 'n'), ('NNS', 'n'), ('JJ', 'adj'), ('RB', 'adv')] )\n",
    "mapRW = dict( [ pair.split() for pair in reservedWords ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isPat(pat):\n",
    "    return pat in pgPatterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxDegree = 9\n",
    "def sentence_to_ngram(words, lemmas, tags, chunks): \n",
    "    return [ (k, k+degree) for k in range(1,len(words)) for degree in range(2, min(maxDegree, len(words)-k+1)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasTwoObjs(tag, chunk):\n",
    "    if chunk[-1] != 'H-NP': return False\n",
    "    return (len(tag) > 1 and tag[0] in pronOBJ) or (len(tag) > 1 and 'DT' in tag[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_to_element(words, lemmas, tags, chunks, i, isHead):\n",
    "    if isHead:                                                          \n",
    "        return mapHead[chunks[i][-1]] if chunks[i][-1] in mapHead else '*'\n",
    "    \n",
    "    if lemmas[i][0] == 'favour' and words[i-1][-1]=='in' and words[i+1][0]=='of': \n",
    "        return 'favour'\n",
    "    \n",
    "    if tags[i][-1] == 'RP' and tags[i-1][-1][:2] == 'VB':                \n",
    "        return '_'\n",
    "    \n",
    "    if tags[i][0][0]=='W' and lemmas[i][-1] in mapRW:                    \n",
    "        return mapRW[lemmas[i][-1]]\n",
    "    \n",
    "    if hasTwoObjs(tags[i], chunks[i]):                                              \n",
    "        return 'n n'\n",
    "    \n",
    "    if tags[i][-1] in mapRest:                            \n",
    "        return mapRest[tags[i][-1]]\n",
    "    if tags[i][-1][:2] in mapRest:                        \n",
    "        return mapRest[tags[i][-1][:2]]\n",
    "    \n",
    "    if chunks[i][-1] in mapHead:                            \n",
    "        return mapHead[chunks[i][-1]].lower()\n",
    "    \n",
    "    if lemmas[i][-1] in pgPreps:                                         \n",
    "        return lemmas[i][-1]\n",
    "    \n",
    "    return lemmas[i][-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplifyPat(pat): \n",
    "    if pat == 'V ,':   return 'V'\n",
    "    elif pat == 'N ,': return 'N'\n",
    "    else: return pat.replace(' _', '').replace('_', ' ').replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_to_pat(words, lemmas, tags, chunks, start, end):\n",
    "    pat, doneHead = [], False\n",
    "    \n",
    "    for i in range(start, end):\n",
    "        isHead = tags[i][-1][0] in ['V', 'N', 'J'] and not doneHead \n",
    "        pat.append( chunk_to_element(words, lemmas, tags, chunks, i, isHead) )\n",
    "        \n",
    "        if isHead: doneHead = True\n",
    "\n",
    "    pat = simplifyPat(' '.join(pat))\n",
    "    return pat if isPat(pat) else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_to_head(words, lemmas, tags, chunks, start, end):\n",
    "    for i in range(start, end):\n",
    "        if tags[i][-1][0] in 'V' and tags[i+1][-1]=='RP':  \n",
    "            return lemmas[i][-1].upper()+ ('_'+lemmas[i+1][-1].upper())\n",
    "        if tags[i][-1][0] in ['V', 'N', 'J']:  \n",
    "            return lemmas[i][-1].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = open('UM-Corpus.en.200k.tagged.txt','r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_patern = []\n",
    "for line in datas:\n",
    "    parse = eval(line.strip())\n",
    "    parse = [ [y.split() for y in x]  for x in parse ]  \n",
    "    sent = ' '.join([' '.join(x) for x in parse[0] ])\n",
    "    #print ('\\n' + sent)\n",
    "    tmp = []\n",
    "    for start, end in sentence_to_ngram(*parse): \n",
    "        pat = ngram_to_pat(*parse, start, end)\n",
    "        if pat:\n",
    "            pos = pat.split(' ')[0]\n",
    "            head = ngram_to_head(*parse, start, end)\n",
    "            if (head+'-'+pos).lower() in akl:\n",
    "                tmp.append((head+'-'+pos, pat, sent))\n",
    "    tmp_patern.append((sent,tmp))\n",
    "                ##print ('%s\\t%s\\t%s' % (head+'-'+pos, pat, sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = defaultdict(Counter)\n",
    "sents = defaultdict(lambda: defaultdict(lambda: []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tgt in tmp_patern:\n",
    "    if tgt[1]:\n",
    "        for p in tgt[1]:\n",
    "            #print(p)\n",
    "            head , patt , sent = p[0] , p[1] , p[2]\n",
    "            counts[head][patt] += 1\n",
    "            sents[head][patt].append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "HiFreWords = open('HiFreWords', 'r').read().split('\\t')\n",
    "Prons = open('prons.txt', 'r').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(str1): return re.findall('[a-z]+', str1.lower()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHighCounts(key, ngrams):\n",
    "    new_counts = list(map(lambda pair: (pair[0], pair[1] * (len(pair[0].split(' '))**1.5)), ngrams.items()))\n",
    "    values = list(map(lambda x: x[1], new_counts))\n",
    "    total, avg, std = np.sum(values), np.mean(values), np.std(values)\n",
    "    remains = filter(lambda pair: pair[1] > avg+std, new_counts)\n",
    "    return remains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(head, sent):\n",
    "    tks = tokens(sent)\n",
    "    bad = sum([t not in HiFreWords and t in Prons for t in tks])\n",
    "    return -bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ability-n\n",
      "N to v\t(468)\tSeiji Uchida , 48 , who lost the ability to walk in a car accident 27 years ago , said he has long dreamed of visiting the picturesque abbey of Mont Saint - Michel , set on a rocky islet in Normandy .\n",
      "\n",
      "value-n\n",
      "N to v\t(16)\tbad : it is a bad idea to abbreviate enumerations ; the user will have no idea which value to use .\n",
      "\n",
      "discuss-v\n",
      "V in n\t(57)\teven in the absence of system failures , there is another risk worth discussing in the above code -- concurrency .\n",
      "V n\t(270)\tshi Shuo , a CAS press officer , told SciDev .Net that there is no plan to reform the academician system , but seminars will be held later this year to discuss it .\n",
      "\n",
      "certain-adj\n",
      "ADJ that\t(50)\tI am much less certain that it benefits a person not to feel sadness or agony about the normal losses we all experience in life if the person is not clinically depressed .\n",
      "ADJ of n\t(23)\tI am only certain of one thing in my life . I knew what I loved to do , and I did what I loved to do .\n",
      "\n",
      "favour-v\n",
      "V n\t(26)\tThesame may have been true in evolution , as merely being dispersed inspace should have been advantageous and should have favoured mobility .\n",
      "\n",
      "classify-v\n",
      "V as n\t(12)\tBupropion is a prescription medication classified as a type of antidepressant . a sustained- release form of bupropion is approved for smoking cessation .\n",
      "V n\t(20)\tbody mass index ( BMI ) is a simple index of weight -for- height that is commonly used to classify overweight and obesity in adults .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target = ['ABILITY-N','VALUE-N','DISCUSS-V','FAVOUR-V','CLASSIFY-V','CERTAIN-ADJ']\n",
    "for head, ngrams in counts.items():\n",
    "    if head not in target:\n",
    "        continue\n",
    "    remains = getHighCounts(head, ngrams)\n",
    "\n",
    "    print(head.lower())\n",
    "    for ptn, ctn in remains:\n",
    "        h_s = max([ (sent, score(head, sent)) for sent in sents[head][ptn] ], key=lambda x: x[1])[0]\n",
    "        print(\"%s\\t(%s)\\t%s\"%(ptn, str(counts[head][ptn]), h_s))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook lab9_106065503.ipynb to script\n",
      "[NbConvertApp] Writing 7490 bytes to lab9_106065503.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script lab9_106065503.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

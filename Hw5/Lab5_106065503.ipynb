{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "def wnTag(pos): return {'noun': 'n', 'verb': 'v', 'adjective': 'a', 'adverb': 'r'}[pos]\n",
    "TF = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "DF = defaultdict(lambda: [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leskOverlap(senseDef, target):\n",
    "    wnidCount = [ (wncat, tf, word, len(DF[word])+1) for word in senseDef \\\n",
    "                                                    for wncat, tf in TF[word].items() \\\n",
    "                                                    if wncat in target.values() ]\n",
    "    res = sorted( [ (word, tf*int(2653/df)) for wnid, tf, word, df in wnidCount], key = lambda x: -x[1])[:5]\n",
    "    feature = {}\n",
    "    for i in res:\n",
    "        feature[i[0]] = i[1]\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = [  line.strip().split('\\t') for line in open('wn.in.evp.cat.txt', 'r') if line.strip() != '' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23194"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isHead(head, word, tag):\n",
    "        try:\n",
    "            return lmtzr.lemmatize(word, tag) == head\n",
    "        except:\n",
    "            return False\n",
    "for wnid, wncat, senseDef, target in training:\n",
    "        head, pos = wnid.split('-')[:2]\n",
    "        tokens = [word for word in words(senseDef) if word not in stop_words]\n",
    "        for word in tokens:\n",
    "            if word != head and not isHead(head, word, pos):\n",
    "                TF[word][wncat] += 1\n",
    "                DF[word] += [] if wncat in DF[word] else [wncat] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['make.v.03', 'edible_fruit.n.01', 'new.a.01', 'implement.n.01']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF['apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class = []\n",
    "features = []\n",
    "for wnid, wncat, senseDef, target in training:\n",
    "    tokens = [word for word in words(senseDef) if word not in stop_words]\n",
    "    Class.append(eval(target))\n",
    "    features.append((leskOverlap(tokens, eval(target)),wncat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'abandoned': 379, 'forsake': 663, 'leave': 639}, 'get_rid_of.v.01')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set,test_set,train_class,test_class = train_test_split(features,Class,test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'day': 675, 'fifth': 662, 'fri': 1326, 'sixth': 663}, 'time_period.n.01')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20874\n",
      "2320\n",
      "20874\n",
      "2320\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "print(len(train_class))\n",
    "print(len(test_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import SklearnClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = SklearnClassifier(LogisticRegression(C=10e5)).train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.440948275862069\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlplab/ting/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    }
   ],
   "source": [
    "hits =0\n",
    "for i in range(len(test_set)):\n",
    "    prob_d = classifier.prob_classify(test_set[i][0])._prob_dict\n",
    "    corr = test_set[i][1]\n",
    "    maxx = max(test_class[i].values(),key=lambda x: prob_d.get(x, 0))\n",
    "    if corr == maxx:\n",
    "        hits +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一種未塞選方法 0.440948275862069\n",
      "第二種塞選後方法 0.6435344827586207\n"
     ]
    }
   ],
   "source": [
    "print('第一種未塞選方法',nltk.classify.accuracy(classifier, test_set))\n",
    "print('第二種塞選後方法',hits/len(test_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Lab5_106065503.ipynb to script\n",
      "[NbConvertApp] Writing 2891 bytes to Lab5_106065503.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Lab5_106065503.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
